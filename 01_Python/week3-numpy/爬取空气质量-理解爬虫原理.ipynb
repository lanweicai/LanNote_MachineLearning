{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬取城市空气质量、理解爬虫原理\n",
    "\n",
    "爬虫是收集数据的常用工具，它的基本操作是：    \n",
    "- 建立网络链接\n",
    "- 专业的工具包，从指定的网页，摘取相应的信息\n",
    "- 爬取的信息，进行处理与保存\n",
    "\n",
    "## 处理城市信息\n",
    "\n",
    "\n",
    "爬取的网站：空气质量指数 (http://www.tianqihoubao.com/aqi/)  \n",
    "\n",
    "\n",
    "HTML基础  \n",
    "超链接 `<a href='链接地址'>链接的文字</a>`  \n",
    "在网页使用F12(或右键审查元素)可以调开控制台，查看网页源代码。   \n",
    "[更多关于html](https://www.runoob.com/html/html-tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html = \"\".join(open('./citychk.txt').readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "m = re.findall('href=\"/aqi/\\w*.html\">.{0,5} ', html)\n",
    "\n",
    "# lambd = lambda x: (x.split('>')[1].strip(), x[11:x.index('.')])\n",
    "# or use function\n",
    "def generate_code(x):\n",
    "    end = x.index('.')\n",
    "    code = x[11:end]\n",
    "    name = x.split('>')[1].strip()\n",
    "    return name,code\n",
    "    \n",
    "city_coding = list(map(generate_code, m))\n",
    "\n",
    "# remove duplicate data\n",
    "print(len(city_coding), len(set(city_coding)))\n",
    "city_coding = set(city_coding)\n",
    "\n",
    "# save\n",
    "with open('./city_coding.txt', 'w') as f:\n",
    "    for line in city_coding:\n",
    "        f.write('\\t'.join(line) + '\\n')\n",
    "print('Saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**项目完整代码 For Reference**  \n",
    "[高民权_中国城市空气质量数据抓取_Github](https://github.com/fortyMiles/ChineseAirConditionCrawler)      \n",
    "Github中的`get_location_info.py`文件对应city_coding的生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 天气数据抓取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取city_coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_city_coding(file='./city_coding.txt'):\n",
    "    city_coding = {}\n",
    "    with open(file) as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            try: \n",
    "                city, coding = line.split('\\t')\n",
    "                city_coding[city.strip()] = coding.strip()\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    return city_coding\n",
    "\n",
    "city_coding = get_city_coding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 拼接成自己想要的URL地址  \n",
    "\n",
    "如果是当前月份可以看到直接使用城市名称即可，如 http://www.tianqihoubao.com/aqi/hangzhou.html  \n",
    "如果查询的是历史月份，可以看到是这种格式 http://www.tianqihoubao.com/aqi/hangzhou-201702.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_url(city_coding, year=None, month=None):\n",
    "    BASE = 'http://www.tianqihoubao.com/aqi/'\n",
    "    city_base_url = BASE + '{}.html'\n",
    "    city_data_base_url = BASE + '{}-{}{}.html'\n",
    "    \n",
    "    if year is not None and month is not None:\n",
    "        month = str(month) if month >= 10 else '0' + str(month)\n",
    "        return city_data_base_url.format(city_coding, year, month)\n",
    "    else:\n",
    "        return city_base_url.format(city_coding)\n",
    "    \n",
    "hangzhou = city_coding['杭州']\n",
    "print(build_url(hangzhou))\n",
    "print(build_url(hangzhou, 2018, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用python进行数据抓取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HTTP协议\n",
    "超文本传输协议（HTTP，HyperText Transfer Protocol）是互联网上应用最为广泛的一种网络协议。所有的www文件都必须遵守这个标准。  \n",
    "\n",
    "HTTP用于客户端和服务器之间的通信。协议中规定了客户端应该按照什么格式给服务器发送请求，同时也约定了服务端返回的响应结果应该是什么格式。    \n",
    "\n",
    "请求访问文本或图像等信息的一端称为客户端，而提供信息响应的一端称为服务器端。 \n",
    "\n",
    "客户端告诉服务器请求访问信息的方法：\n",
    "- Get 获得内容\n",
    "- Post 提交表单来爬取需要登录才能获得数据的网站\n",
    "- put 传输文件  \n",
    "\n",
    "更多参考：\n",
    "[HTTP请求状态](https://www.runoob.com/http/http-status-codes.html)  \n",
    "了解200 404 503\n",
    " - 200 OK      //客户端请求成功\n",
    " - 404 Not Found  //请求资源不存在，eg：输入了错误的URL\n",
    " - 503 Server Unavailable  //服务器当前不能处理客户端的请求，一段时间后可能恢复正常。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requests\n",
    "纯粹HTML格式的网页通常被称为静态网页，静态网页的数据比较容易获取。   \n",
    "在静态网页抓取中，有一个强大的Requests库能够让你轻易地发送HTTP请求。    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在终端上安装 Requests\n",
    "pip install requents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取响应内容\n",
    "import requests\n",
    "r = requests.get('https://www.crummy.com/')\n",
    "# \n",
    "print('文本编码：（服务器使用的文本编码）', r.encoding)\n",
    "\n",
    "print('响应状态码：（200表示成功）', r.status_code)\n",
    "\n",
    "print('字符串方式的响应体：（服务器响应的内容）', r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拓展知识：\n",
    "- [Unicode和UTF-8有什么区别?(盛世唐朝回答)](https://www.zhihu.com/question/23374078)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用 BeautifulSoup 解析网页  \n",
    "BeautifulSoup 是一个工具箱，可以从HTML或XML文件中提取数据。   \n",
    "参考：[Beautiful Soup 4.2.0 文档](https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**首先安装包**  \n",
    "\n",
    "``` bash\n",
    "pip install bs4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "# 把网页响应的字符串转化为soup对象，然后使用soup库的功能\n",
    "soup = BeautifulSoup(r.text,'html.parser')\n",
    "# 获得标题内容tti\n",
    "print(soup.title)\n",
    "# 可以对代码进行美化\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用 lxml 解析网页\n",
    "介绍一个比较流行的解析库 lxml。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 导入lxml中的etree模块\n",
    "from lxml import etree\n",
    "\n",
    "# 调用etree模块中的HTML()类，将text作为参数传入\n",
    "html = etree.HTML(r.text)\n",
    "\n",
    "print(type(html))\n",
    "result = etree.tostring(html)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tostring()`方法将解析后的HTML文档输出，可以看到输出的类型为bytes，我们可以利用`decode()`方法将其转化为str类型输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般用 `//` 开头选择符合要求的所有节点，比如对于上面的html，我们选取所有`<link>`节点。    \n",
    "[更多参考](https://www.cnblogs.com/baowee/p/11364941.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_result = html.xpath('//link')\n",
    "print(link_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# //div 选取了HTML文档中所有<div>节点，/ 表示选取当前节点的直接子节点。\n",
    "div_a_result = html.xpath('//div/a')\n",
    "print(div_a_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，通过F12查看hangzhou-201805.html请求，可以看到`Content-Type: text/html; charset=gb2312` 所示使用的是GBK编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "hangzhou = city_coding['杭州']\n",
    "url = build_url(hangzhou, 2018, 5)\n",
    "\n",
    "# 发送请求\n",
    "# get post\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "# 查看相关信息\n",
    "# help(reponse)\n",
    "\n",
    "print(response.status_code, response.ok)\n",
    "\n",
    "# 打印返回的结果\n",
    "print(response.encoding)\n",
    "html = response.text\n",
    "soup = BeautifulSoup(html)\n",
    "\n",
    "# 一些属性 \n",
    "# 网页的title\n",
    "print(soup.title)\n",
    "# 网页的文本\n",
    "# print(soup.text)\n",
    "\n",
    "# 查找属性\n",
    "data_table = soup.find_all('table')\n",
    "print(len(data_table))\n",
    "# print(data_table)\n",
    "\n",
    "# 既然只有一个table\n",
    "# 可以使用下面\n",
    "data_table = soup.table\n",
    "\n",
    "# 然后进行更加细化的数据分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看下data_table内容\n",
    "# print(data_table)\n",
    "\n",
    "# data.contents 将对象下的元素都获取得到 返回List\n",
    "# 可以看到第一行是表头 \n",
    "# 并且隔一行有一个\\n元素\n",
    "\n",
    "name_index = 1\n",
    "content = data_table.contents[name_index:]\n",
    "\n",
    "result = []\n",
    "for index, c in enumerate(content[::2]):\n",
    "    if index == 0:\n",
    "        result.append(tuple(['city'] + c.text.split()))\n",
    "    else:\n",
    "        result.append(tuple([hangzhou] + c.text.split()))\n",
    "        \n",
    "print(len(result), result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 整体代码总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_city_coding(file='./city_coding.txt'):\n",
    "    city_coding = {}\n",
    "    with open(file) as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            try: \n",
    "                city, coding = line.split('\\t')\n",
    "                city_coding[city.strip()] = coding.strip()\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    return city_coding\n",
    "\n",
    "def build_url(city_coding, year=None, month=None):\n",
    "    BASE = 'http://www.tianqihoubao.com/aqi/'\n",
    "    city_base_url = BASE + '{}.html'\n",
    "    city_data_base_url = BASE + '{}-{}{}.html'\n",
    "    \n",
    "    if year is not None and month is not None:\n",
    "        month = str(month) if month >= 10 else '0' + str(month)\n",
    "        return city_data_base_url.format(city_coding, year, month)\n",
    "    else:\n",
    "        return city_base_url.format(city_coding)\n",
    "    \n",
    "def parse(url, city_name):\n",
    "    response = requests.get(url)\n",
    "    if response.ok:\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html)\n",
    "        data_table = soup.table\n",
    "\n",
    "        name_index = 1\n",
    "        content = data_table.contents[name_index:]\n",
    "\n",
    "        result = []\n",
    "        for index, c in enumerate(content[::2]):\n",
    "            if index == 0:\n",
    "                result.append(tuple(['city'] + c.text.split()))\n",
    "            else:\n",
    "                result.append(tuple([city_name] + c.text.split()))\n",
    "        \n",
    "        return result\n",
    "    else:\n",
    "        print('Network Error:', response.status_code)\n",
    "\n",
    "city_coding = get_city_coding()\n",
    "want_city = city_coding['杭州']\n",
    "url = build_url(want_city, 2019, 9)\n",
    "result = parse(url, want_city)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将抓取的数据进行保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "def save_csv(file, data):\n",
    "    if data == None or len(data) == 1: return\n",
    "    if os.path.exists(file):\n",
    "        with open(file, 'a') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(data[1:])\n",
    "    else:\n",
    "        with open(file, 'w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(data)\n",
    "\n",
    "save_csv(f'./{want_city}.csv', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始抓取所有数据\n",
    "- **Refrain from running in Jupyter!!**\n",
    "- **在Jupyter中慎重使用！！**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allcities = list(get_city_coding().keys())\n",
    "\n",
    "file = r'./allcity_2019.csv'\n",
    "for city in allcities:\n",
    "    city_code = city_coding[city]\n",
    "    for year in range(2019, 2018,-1):\n",
    "        for month in range(1,13):\n",
    "            url = build_url(city_code, year, month)\n",
    "            data = parse(url, city_code)\n",
    "            print(f'\\r{city}\\t{year}-{month} {len(result)}', end='')\n",
    "            save_csv(file, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**项目完整代码 For Reference**  \n",
    "[高民权_中国城市空气质量数据抓取_Github](https://github.com/fortyMiles/ChineseAirConditionCrawler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def get_city_coding():\n",
    "    CITY_CODIN = './city_coding.txt'\n",
    "    city_coding = {}\n",
    "    with open(CITY_CODIN, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            try:\n",
    "                city, coding = line.split('\\t')\n",
    "                city_coding[city.strip()] = coding.strip()\n",
    "            except ValueError as e:\n",
    "                continue\n",
    "\n",
    "    return city_coding\n",
    "\n",
    "\n",
    "def build_url(city_coding, year=None, month=None):\n",
    "    BASE = 'http://www.tianqihoubao.com/aqi/'\n",
    "    city_base_url = BASE + \"{}.html\"\n",
    "    city_data_base_url = BASE + \"{}-{}{}.html\"\n",
    "\n",
    "    if year is not None and month is not None:\n",
    "        month = str(month) if month >= 10 else '0' + str(month)\n",
    "        return city_data_base_url.format(city_coding, year, month)\n",
    "    else:\n",
    "        return city_base_url.format(city_coding)\n",
    "\n",
    "\n",
    "def get_from_http(city_coding, year=None, month=None):\n",
    "    '''\n",
    "    \n",
    "    :param city_coding: city Chinese Name, e.g hangzhou \n",
    "    :param year: e.g 2016\n",
    "    :param month: e.g 10\n",
    "    :param day:  e.g 5\n",
    "    :return: {\n",
    "                'city': string,\n",
    "                'air_conditions': [air_condition]\n",
    "             }\n",
    "             \n",
    "             air_condition = (Date, AQI, Pm2.5, Pm10, No2, So2, Co, O3)\n",
    "             \n",
    "    '''\n",
    "\n",
    "    url = build_url(city_coding, year, month)\n",
    "\n",
    "    content = get_some_day_air_condition(city_coding, url)\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "def get_some_day_air_condition(city_coding, url):\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        if r.status_code == 200:\n",
    "            r.encoding = 'GBK'\n",
    "            html_file = r.text\n",
    "            soup = BeautifulSoup(html_file, 'html.parser')\n",
    "\n",
    "            data_table = soup.find_all('table')\n",
    "            data_table = soup.table\n",
    "\n",
    "            return parse(city_coding, data_table)\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print('connnect error')\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse(city_coding, data):\n",
    "    #data.contents[1].text.split()\n",
    "    #data.contents[3].text.split()\n",
    "    name_index = 1\n",
    "    content = data.contents[name_index:]\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for index, c in enumerate(content[::2]):\n",
    "        if index == 0:\n",
    "            result.append(tuple(['city'] + c.text.split()))\n",
    "        else:\n",
    "            result.append(tuple([city_coding] + c.text.split()))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #get_from_http('杭州', 2015, 10, 6)\n",
    "    city_coding = get_city_coding()\n",
    "    assert city_coding['杭州'] == 'hangzhou'\n",
    "\n",
    "    hangzhou = city_coding['杭州']\n",
    "\n",
    "    print('testing')\n",
    "\n",
    "    assert build_url(hangzhou, 2016, 5) == \"http://www.tianqihoubao.com/aqi/hangzhou-201605.html\"\n",
    "    assert build_url(hangzhou, 2016) == \"http://www.tianqihoubao.com/aqi/hangzhou.html\"\n",
    "    assert build_url(hangzhou) == \"http://www.tianqihoubao.com/aqi/hangzhou.html\"\n",
    "\n",
    "    assert get_some_day_air_condition(\"hanghzhou\", \"http://www.tianqihoubao.com/aqi/hangzhou-201605.html\") is not None\n",
    "\n",
    "    data = get_some_day_air_condition(\"hangzhou\", \"http://www.tianqihoubao.com/aqi/hangzhou-201605.html\")\n",
    "    #print(data)\n",
    "\n",
    "    city_data = get_from_http('hangzhou', 2015, 10)\n",
    "    print(city_data)\n",
    "\n",
    "    print('test done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意 get_city_coding 中添加文件编码为UTF-8**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拓展知识：\n",
    "- [哪些 Python 库让你相见恨晚？](https://www.zhihu.com/question/24590883)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
